{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea3e9044",
   "metadata": {},
   "source": [
    "# English quenya translation using a transformer - Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372476ce",
   "metadata": {},
   "source": [
    "## Main page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dd796ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import nltk\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "from model import Transformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Importing all of the necessary libraries\n",
    "\n",
    "url = \"https://eldamo.org/content/phrase-indexes/phrases-q.html\"\n",
    "r = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ce8c80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c975409",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_english = soup.select('li')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8040bb7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A anamelda na ar ilyan  “A is dearest of all”\\n               '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows_english[50].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "394c474d",
   "metadata": {},
   "outputs": [],
   "source": [
    "quenya_sentences =[]\n",
    "for row in rows_english:\n",
    "    quenya_sentences.append(row.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f53dcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = quenya_sentences[15:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54f1c390",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_english = []\n",
    "sentences_quenya = []\n",
    "for sentence in sentences:\n",
    "    compteur = 0\n",
    "    for i in range(len(sentence)): #separating the lines to get the english and quenya parallel sentences\n",
    "        if sentence[i]==\"“\": \n",
    "            beginning = i+1\n",
    "        if sentence[i]==\"”\":\n",
    "            ending = i #allows to find where the english sentence is located in the string\n",
    "            last_ending = ending\n",
    "            compteur+=1\n",
    "            sentences_english.append(sentence[beginning:ending])\n",
    "            if compteur==1:\n",
    "                sentences_quenya.append(sentence[:beginning])\n",
    "            else:\n",
    "                sentences_quenya.append(sentence[last_ending:beginning])\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453826ac",
   "metadata": {},
   "source": [
    "## Poems & prayers found on the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "034e305a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_url = [\"https://eldamo.org/content/words/word-2555725393.html\", \"https://eldamo.org/content/words/word-2245526111.html\",\n",
    "            \"https://eldamo.org/content/words/word-671674147.html\", \"https://eldamo.org/content/words/word-311699583.html\",\n",
    "            \"https://eldamo.org/content/words/word-2920398593.html\", \"https://eldamo.org/content/words/word-3295893985.html\",\n",
    "            \"https://eldamo.org/content/words/word-2124111669.html\", \"https://eldamo.org/content/words/word-4161205007.html\",\n",
    "            \"https://eldamo.org/content/words/word-436003197.html\", \"https://eldamo.org/content/words/word-2774144071.html\",\n",
    "            \"https://eldamo.org/content/words/word-3330342599.html\", \"https://eldamo.org/content/words/word-1216507117.html\",\n",
    "            \"https://eldamo.org/content/words/word-2721399773.html\", \"https://eldamo.org/content/words/word-1235857611.html\"]\n",
    "for url in list_url:\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    rows_english = soup.select('td')\n",
    "    for i in range(0,len(rows_english)):\n",
    "        if i%2==0:\n",
    "            sentences_quenya.append(rows_english[i].text)\n",
    "        else:\n",
    "            sentences_english.append(rows_english[i].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067d85d4",
   "metadata": {},
   "source": [
    "## Dictionary pulled from Eldamo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa9d46d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://eldamo.org/content/vocabulary-indexes/vocabulary-words-nq.html?neo\"\n",
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "rows = soup.select('dt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81348d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for row in rows:\n",
    "    words.append(row.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b436769f",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_english = []\n",
    "words_quenya = []\n",
    "for word in words:\n",
    "    compteur=0\n",
    "    for i in range(len(word)):\n",
    "        if ((word[i]==\" \") | (word[i]==\"-\"))&(compteur==0)&(i!=0): #separating the data according to its structure on the website\n",
    "            ending_quenya = i\n",
    "            words_quenya.append(word[:ending_quenya])\n",
    "            compteur+=1\n",
    "        if word[i]==\"“\":\n",
    "            beginning = i+1 \n",
    "        if word[i]==\"”\":\n",
    "            ending = i\n",
    "            words_english.append(word[beginning:ending])\n",
    "            break\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20167db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_english = np.concatenate([sentences_english, words_english])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa744ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_quenya = np.concatenate([sentences_quenya, words_quenya])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "abee0d4b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "list_english = []\n",
    "list_quenya = []\n",
    "for sentence in sentences_english:\n",
    "    list_english.append(list(sentence))\n",
    "for sentence in sentences_quenya:\n",
    "    list_quenya.append(list(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b6b1f40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 0\n",
    "for i in range(len(sentences_english)): #finding the maximum length for a sentence\n",
    "    if len(list_english[i])>m: \n",
    "        m = len(list_english[i])\n",
    "    if len(list_quenya[i])>m:\n",
    "        m = len(list_quenya[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734cb443",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "569110e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "char2index = {} #preparing the dictionaries for tokenization\n",
    "index2char = {}\n",
    "counter = 1\n",
    "for i in range(len(sentences_english)):\n",
    "    sent_english = sentences_english[i]\n",
    "    sent_quenya = sentences_quenya[i]\n",
    "    for w in sent_english:\n",
    "        if w not in char2index:\n",
    "            counter+=1\n",
    "            char2index[w] = counter\n",
    "            index2char[counter] = w\n",
    "    for w in sent_quenya:\n",
    "        if w not in char2index:\n",
    "            counter+=1\n",
    "            char2index[w] = counter\n",
    "            index2char[counter] = w\n",
    "char2index['<EOS>'] = counter+1\n",
    "index2char[counter+1] = '<EOS>' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "669f3998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5574, 147)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_en = np.zeros([len(list_english), m+1]) #preparing the dataset. 0 is the padding token.\n",
    "data_quenya = np.zeros([len(list_english), m+1])\n",
    "data_quenya.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8db10384",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(list_english)): #adding <EOS> token at the end of each sentence\n",
    "    for j in range(len(list_english[i])):\n",
    "        data_en[i,j] = char2index[list_english[i][j]]\n",
    "    data_en[i,len(list_english[i])] = char2index['<EOS>']\n",
    "    for j in range(len(list_quenya[i])):\n",
    "        data_quenya[i,j] = char2index[list_quenya[i][j]]\n",
    "    data_quenya[i,len(list_quenya[i])] = char2index['<EOS>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8729a1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros(147) #checking for empty sentences which cause an issue if fed to the multihead attention layers with a padding mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6fc6a5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "L=[]\n",
    "for d in range(len(data_quenya)):\n",
    "    if np.array_equal(a,data_quenya[d]):\n",
    "        L.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "05b32c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_en = np.delete(data_en,L,0)\n",
    "data_quenya = np.delete(data_quenya,L,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0a0db0",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "11620b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a295e2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOC_SIZE = len(char2index)+2\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 4\n",
    "HID_DIM = 1024\n",
    "BATCH_SIZE = 40\n",
    "NUM_ENCODER_LAYERS = 4\n",
    "NUM_DECODER_LAYERS = 4 #model hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f4a32f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE, NHEAD, HID_DIM, 0.01, VOC_SIZE)#model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dbacac8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = transformer.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "52ce275e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "en_train, en_test, qu_train, qu_test = train_test_split(data_en, data_quenya, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4bf8a7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_train = torch.Tensor(en_train).long() \n",
    "en_test = torch.Tensor(en_test).long()\n",
    "qu_train = torch.Tensor(qu_train).long()\n",
    "qu_test = torch.Tensor(qu_test).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d3233062",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(en_train, qu_train)\n",
    "test_dataset = TensorDataset(en_test, qu_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ac6616f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16def618",
   "metadata": {},
   "source": [
    "## Masks creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5eebb3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positionalEncoding(length, embed_dim): #positionalEncoding as described in the original paper\n",
    "    angles = np.array([[pos/10000**(2*(i//2)) for i in range(embed_dim)] for pos in range(length)])\n",
    "    pos_encoding = np.zeros((length, embed_dim))\n",
    "    pos_encoding[:,0::2] = np.sin(angles[:,0::2])\n",
    "    pos_encoding[:,1::2] = np.cos(angles[:,1::2])\n",
    "    return torch.from_numpy(pos_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4e982ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size): #attention mask for multi head attention\n",
    "    mask = torch.triu(torch.ones(size, size), diagonal=1).to(torch.bool)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "52e6bab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_mask(x):\n",
    "    return x==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "05af6e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_encoding = positionalEncoding(m+1, EMB_SIZE).to(DEVICE)\n",
    "attn_mask = create_look_ahead_mask(m+1).to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5c7a5d",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6cb417",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Batch [5/112], Loss: 4.7044\n",
      "Epoch [1/10], Batch [10/112], Loss: 4.7013\n",
      "Epoch [1/10], Batch [15/112], Loss: 4.7009\n",
      "Epoch [1/10], Batch [20/112], Loss: 4.7029\n",
      "Epoch [1/10], Batch [25/112], Loss: 4.7039\n",
      "Epoch [1/10], Batch [30/112], Loss: 4.7025\n",
      "Epoch [1/10], Batch [35/112], Loss: 4.7015\n",
      "Epoch [1/10], Batch [40/112], Loss: 4.7009\n",
      "Epoch [1/10], Batch [45/112], Loss: 4.7012\n",
      "Epoch [1/10], Batch [50/112], Loss: 4.7015\n",
      "Epoch [1/10], Batch [55/112], Loss: 4.7033\n",
      "Epoch [1/10], Batch [60/112], Loss: 4.7056\n",
      "Epoch [1/10], Batch [65/112], Loss: 4.7020\n",
      "Epoch [1/10], Batch [70/112], Loss: 4.7050\n",
      "Epoch [1/10], Batch [75/112], Loss: 4.7020\n",
      "Epoch [1/10], Batch [80/112], Loss: 4.7008\n",
      "Epoch [1/10], Batch [85/112], Loss: 4.7003\n",
      "Epoch [1/10], Batch [90/112], Loss: 4.7019\n",
      "Epoch [1/10], Batch [95/112], Loss: 4.7024\n",
      "Epoch [1/10], Batch [100/112], Loss: 4.7009\n",
      "Epoch [1/10], Batch [105/112], Loss: 4.7059\n",
      "Epoch [1/10], Batch [110/112], Loss: 4.7026\n",
      "Epoch [2/10], Batch [5/112], Loss: 4.7044\n",
      "Epoch [2/10], Batch [10/112], Loss: 4.7005\n",
      "Epoch [2/10], Batch [15/112], Loss: 4.7002\n",
      "Epoch [2/10], Batch [20/112], Loss: 4.7026\n",
      "Epoch [2/10], Batch [25/112], Loss: 4.7035\n",
      "Epoch [2/10], Batch [30/112], Loss: 4.7025\n",
      "Epoch [2/10], Batch [35/112], Loss: 4.7011\n",
      "Epoch [2/10], Batch [40/112], Loss: 4.7003\n",
      "Epoch [2/10], Batch [45/112], Loss: 4.7008\n",
      "Epoch [2/10], Batch [50/112], Loss: 4.7012\n",
      "Epoch [2/10], Batch [55/112], Loss: 4.7031\n",
      "Epoch [2/10], Batch [60/112], Loss: 4.7053\n",
      "Epoch [2/10], Batch [65/112], Loss: 4.7017\n",
      "Epoch [2/10], Batch [70/112], Loss: 4.7048\n",
      "Epoch [2/10], Batch [75/112], Loss: 4.7017\n",
      "Epoch [2/10], Batch [80/112], Loss: 4.7006\n",
      "Epoch [2/10], Batch [85/112], Loss: 4.7002\n",
      "Epoch [2/10], Batch [90/112], Loss: 4.7019\n",
      "Epoch [2/10], Batch [95/112], Loss: 4.7025\n",
      "Epoch [2/10], Batch [100/112], Loss: 4.7012\n",
      "Epoch [2/10], Batch [105/112], Loss: 4.7056\n",
      "Epoch [2/10], Batch [110/112], Loss: 4.7026\n",
      "Epoch [3/10], Batch [5/112], Loss: 4.7043\n",
      "Epoch [3/10], Batch [10/112], Loss: 4.7005\n",
      "Epoch [3/10], Batch [15/112], Loss: 4.7000\n",
      "Epoch [3/10], Batch [20/112], Loss: 4.7025\n",
      "Epoch [3/10], Batch [25/112], Loss: 4.7031\n",
      "Epoch [3/10], Batch [30/112], Loss: 4.7016\n",
      "Epoch [3/10], Batch [35/112], Loss: 4.7008\n",
      "Epoch [3/10], Batch [40/112], Loss: 4.7000\n",
      "Epoch [3/10], Batch [45/112], Loss: 4.7007\n",
      "Epoch [3/10], Batch [50/112], Loss: 4.7008\n",
      "Epoch [3/10], Batch [55/112], Loss: 4.7031\n",
      "Epoch [3/10], Batch [60/112], Loss: 4.7052\n",
      "Epoch [3/10], Batch [65/112], Loss: 4.7019\n",
      "Epoch [3/10], Batch [70/112], Loss: 4.7044\n",
      "Epoch [3/10], Batch [75/112], Loss: 4.7016\n",
      "Epoch [3/10], Batch [80/112], Loss: 4.7002\n",
      "Epoch [3/10], Batch [85/112], Loss: 4.6999\n",
      "Epoch [3/10], Batch [90/112], Loss: 4.7015\n",
      "Epoch [3/10], Batch [95/112], Loss: 4.7024\n",
      "Epoch [3/10], Batch [100/112], Loss: 4.7004\n",
      "Epoch [3/10], Batch [105/112], Loss: 4.7057\n",
      "Epoch [3/10], Batch [110/112], Loss: 4.7022\n",
      "Epoch [4/10], Batch [5/112], Loss: 4.7039\n",
      "Epoch [4/10], Batch [10/112], Loss: 4.7006\n",
      "Epoch [4/10], Batch [15/112], Loss: 4.7000\n",
      "Epoch [4/10], Batch [20/112], Loss: 4.7028\n",
      "Epoch [4/10], Batch [25/112], Loss: 4.7031\n",
      "Epoch [4/10], Batch [30/112], Loss: 4.7020\n",
      "Epoch [4/10], Batch [35/112], Loss: 4.7014\n",
      "Epoch [4/10], Batch [40/112], Loss: 4.7002\n",
      "Epoch [4/10], Batch [45/112], Loss: 4.7005\n",
      "Epoch [4/10], Batch [50/112], Loss: 4.7009\n",
      "Epoch [4/10], Batch [55/112], Loss: 4.7028\n",
      "Epoch [4/10], Batch [60/112], Loss: 4.7053\n",
      "Epoch [4/10], Batch [65/112], Loss: 4.7016\n",
      "Epoch [4/10], Batch [70/112], Loss: 4.7044\n",
      "Epoch [4/10], Batch [75/112], Loss: 4.7015\n",
      "Epoch [4/10], Batch [80/112], Loss: 4.7008\n",
      "Epoch [4/10], Batch [85/112], Loss: 4.6994\n",
      "Epoch [4/10], Batch [90/112], Loss: 4.7015\n",
      "Epoch [4/10], Batch [95/112], Loss: 4.7020\n",
      "Epoch [4/10], Batch [100/112], Loss: 4.7003\n",
      "Epoch [4/10], Batch [105/112], Loss: 4.7055\n",
      "Epoch [4/10], Batch [110/112], Loss: 4.7020\n",
      "Epoch [5/10], Batch [5/112], Loss: 4.7038\n",
      "Epoch [5/10], Batch [10/112], Loss: 4.7003\n",
      "Epoch [5/10], Batch [15/112], Loss: 4.6998\n",
      "Epoch [5/10], Batch [20/112], Loss: 4.7021\n",
      "Epoch [5/10], Batch [25/112], Loss: 4.7032\n",
      "Epoch [5/10], Batch [30/112], Loss: 4.7027\n",
      "Epoch [5/10], Batch [35/112], Loss: 4.7007\n",
      "Epoch [5/10], Batch [40/112], Loss: 4.7000\n",
      "Epoch [5/10], Batch [45/112], Loss: 4.7006\n",
      "Epoch [5/10], Batch [50/112], Loss: 4.7009\n",
      "Epoch [5/10], Batch [55/112], Loss: 4.7030\n",
      "Epoch [5/10], Batch [60/112], Loss: 4.7051\n",
      "Epoch [5/10], Batch [65/112], Loss: 4.7015\n",
      "Epoch [5/10], Batch [70/112], Loss: 4.7045\n",
      "Epoch [5/10], Batch [75/112], Loss: 4.7014\n",
      "Epoch [5/10], Batch [80/112], Loss: 4.7000\n",
      "Epoch [5/10], Batch [85/112], Loss: 4.6995\n",
      "Epoch [5/10], Batch [90/112], Loss: 4.7014\n",
      "Epoch [5/10], Batch [95/112], Loss: 4.7021\n",
      "Epoch [5/10], Batch [100/112], Loss: 4.7001\n",
      "Epoch [5/10], Batch [105/112], Loss: 4.7054\n",
      "Epoch [5/10], Batch [110/112], Loss: 4.7021\n",
      "Epoch [6/10], Batch [5/112], Loss: 4.7038\n",
      "Epoch [6/10], Batch [10/112], Loss: 4.7005\n",
      "Epoch [6/10], Batch [15/112], Loss: 4.6999\n",
      "Epoch [6/10], Batch [20/112], Loss: 4.7021\n",
      "Epoch [6/10], Batch [25/112], Loss: 4.7032\n",
      "Epoch [6/10], Batch [30/112], Loss: 4.7015\n",
      "Epoch [6/10], Batch [35/112], Loss: 4.7010\n",
      "Epoch [6/10], Batch [40/112], Loss: 4.6999\n",
      "Epoch [6/10], Batch [45/112], Loss: 4.6999\n",
      "Epoch [6/10], Batch [50/112], Loss: 4.7002\n",
      "Epoch [6/10], Batch [55/112], Loss: 4.7024\n",
      "Epoch [6/10], Batch [60/112], Loss: 4.7047\n",
      "Epoch [6/10], Batch [65/112], Loss: 4.7016\n",
      "Epoch [6/10], Batch [70/112], Loss: 4.7043\n",
      "Epoch [6/10], Batch [75/112], Loss: 4.7013\n",
      "Epoch [6/10], Batch [80/112], Loss: 4.6999\n",
      "Epoch [6/10], Batch [85/112], Loss: 4.6991\n",
      "Epoch [6/10], Batch [90/112], Loss: 4.7008\n",
      "Epoch [6/10], Batch [95/112], Loss: 4.7013\n",
      "Epoch [6/10], Batch [100/112], Loss: 4.7004\n",
      "Epoch [6/10], Batch [105/112], Loss: 4.7053\n",
      "Epoch [6/10], Batch [110/112], Loss: 4.7019\n",
      "Epoch [7/10], Batch [5/112], Loss: 4.7036\n",
      "Epoch [7/10], Batch [10/112], Loss: 4.7004\n",
      "Epoch [7/10], Batch [15/112], Loss: 4.6998\n",
      "Epoch [7/10], Batch [20/112], Loss: 4.7019\n",
      "Epoch [7/10], Batch [25/112], Loss: 4.7028\n",
      "Epoch [7/10], Batch [30/112], Loss: 4.7015\n",
      "Epoch [7/10], Batch [35/112], Loss: 4.7005\n",
      "Epoch [7/10], Batch [40/112], Loss: 4.6998\n",
      "Epoch [7/10], Batch [45/112], Loss: 4.7003\n",
      "Epoch [7/10], Batch [50/112], Loss: 4.7001\n",
      "Epoch [7/10], Batch [55/112], Loss: 4.7024\n",
      "Epoch [7/10], Batch [60/112], Loss: 4.7049\n",
      "Epoch [7/10], Batch [65/112], Loss: 4.7012\n",
      "Epoch [7/10], Batch [70/112], Loss: 4.7042\n",
      "Epoch [7/10], Batch [75/112], Loss: 4.7013\n",
      "Epoch [7/10], Batch [80/112], Loss: 4.7000\n",
      "Epoch [7/10], Batch [85/112], Loss: 4.6990\n",
      "Epoch [7/10], Batch [90/112], Loss: 4.7011\n",
      "Epoch [7/10], Batch [95/112], Loss: 4.7017\n",
      "Epoch [7/10], Batch [100/112], Loss: 4.7000\n",
      "Epoch [7/10], Batch [105/112], Loss: 4.7052\n",
      "Epoch [7/10], Batch [110/112], Loss: 4.7017\n",
      "Epoch [8/10], Batch [5/112], Loss: 4.7038\n",
      "Epoch [8/10], Batch [10/112], Loss: 4.7000\n",
      "Epoch [8/10], Batch [15/112], Loss: 4.6996\n",
      "Epoch [8/10], Batch [20/112], Loss: 4.7018\n",
      "Epoch [8/10], Batch [25/112], Loss: 4.7030\n",
      "Epoch [8/10], Batch [30/112], Loss: 4.7015\n",
      "Epoch [8/10], Batch [35/112], Loss: 4.7001\n",
      "Epoch [8/10], Batch [40/112], Loss: 4.6998\n",
      "Epoch [8/10], Batch [45/112], Loss: 4.7000\n",
      "Epoch [8/10], Batch [50/112], Loss: 4.7000\n",
      "Epoch [8/10], Batch [55/112], Loss: 4.7025\n",
      "Epoch [8/10], Batch [60/112], Loss: 4.7050\n",
      "Epoch [8/10], Batch [65/112], Loss: 4.7012\n",
      "Epoch [8/10], Batch [70/112], Loss: 4.7044\n",
      "Epoch [8/10], Batch [75/112], Loss: 4.7009\n",
      "Epoch [8/10], Batch [80/112], Loss: 4.7002\n",
      "Epoch [8/10], Batch [85/112], Loss: 4.6991\n",
      "Epoch [8/10], Batch [90/112], Loss: 4.7011\n",
      "Epoch [8/10], Batch [95/112], Loss: 4.7016\n",
      "Epoch [8/10], Batch [100/112], Loss: 4.7000\n",
      "Epoch [8/10], Batch [105/112], Loss: 4.7052\n",
      "Epoch [8/10], Batch [110/112], Loss: 4.7015\n",
      "Epoch [9/10], Batch [5/112], Loss: 4.7034\n",
      "Epoch [9/10], Batch [10/112], Loss: 4.7000\n",
      "Epoch [9/10], Batch [15/112], Loss: 4.6996\n",
      "Epoch [9/10], Batch [20/112], Loss: 4.7016\n",
      "Epoch [9/10], Batch [25/112], Loss: 4.7030\n",
      "Epoch [9/10], Batch [30/112], Loss: 4.7011\n",
      "Epoch [9/10], Batch [35/112], Loss: 4.7004\n",
      "Epoch [9/10], Batch [40/112], Loss: 4.6996\n",
      "Epoch [9/10], Batch [45/112], Loss: 4.7004\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()#training loop\n",
    "transformer.train()\n",
    "num_epochs = 10\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=1e-5)\n",
    "torch.autograd.set_detect_anomaly(True) \n",
    "for epoch in range(num_epochs):\n",
    "    # Iterate over the batches in the train_dataloader\n",
    "    for batch_idx, batch in enumerate(train_dataloader):\n",
    "        # Get the batch of input sentences and labels\n",
    "        sentences = batch[0].to(DEVICE)\n",
    "        labels = batch[1].to(DEVICE)\n",
    "        src_padding_mask = padding_mask(sentences).to(DEVICE)\n",
    "        tgt_padding_mask = padding_mask(labels).to(DEVICE)\n",
    "        # Reset the gradients\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = transformer(sentences, labels, src_padding_mask, tgt_padding_mask, attn_mask, pos_encoding)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs.view(-1,VOC_SIZE,m+1), labels)\n",
    "        torch.nn.utils.clip_grad_norm_(transformer.parameters(), 0.5)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print the loss every 5 batches\n",
    "        if (batch_idx + 1) % 5 == 0:\n",
    "            print('Epoch [{}/{}], Batch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, batch_idx+1, len(train_dataloader), loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5c4b94",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5d90fbde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): Encoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (mha): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (fullyconnected1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (fullyconnected2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): EncoderLayer(\n",
       "        (mha): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (fullyconnected1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (fullyconnected2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): EncoderLayer(\n",
       "        (mha): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (fullyconnected1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (fullyconnected2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): EncoderLayer(\n",
       "        (mha): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (fullyconnected1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (fullyconnected2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (mha1): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mha2): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (fullyconnected1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (fullyconnected2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (mha1): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mha2): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (fullyconnected1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (fullyconnected2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): DecoderLayer(\n",
       "        (mha1): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mha2): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (fullyconnected1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (fullyconnected2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): DecoderLayer(\n",
       "        (mha1): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mha2): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (fullyconnected1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (fullyconnected2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=512, out_features=115, bias=True)\n",
       "  (src_embedding): Embedding(115, 512)\n",
       "  (tgt_embedding): Embedding(115, 512)\n",
       "  (softmax_layer): Softmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.train(mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1627ca9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = np.zeros(m+1) #inference\n",
    "output_sentence = np.zeros(m+1)\n",
    "generated_sentence = []\n",
    "my_sentence = \"This is a test sentence\"\n",
    "for i in range(len(my_sentence)):\n",
    "    input_sentence[i] = char2index[my_sentence[i]] #tokenization of the input sentence and adding EOS token\n",
    "input_sentence[len(my_sentence)]=char2index['<EOS>']\n",
    "input_sentence = torch.Tensor(input_sentence).long().to(DEVICE)\n",
    "output_sentence = torch.Tensor(output_sentence).long().to(DEVICE)\n",
    "output_sentence[0]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "66828ce4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0], device='cuda:0')\n",
      "tensor([ 0, 31,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0], device='cuda:0')\n",
      "tensor([ 0, 31, 91,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0], device='cuda:0')\n",
      "tensor([ 0, 31, 91, 81,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0], device='cuda:0')\n",
      "tensor([ 0, 31, 91, 81,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0], device='cuda:0')\n",
      "tensor([ 0, 31, 91, 81,  2, 15,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0], device='cuda:0')\n",
      "tensor([ 0, 31, 91, 81,  2, 15, 92,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0], device='cuda:0')\n",
      "tensor([ 0, 31, 91, 81,  2, 15, 92, 92,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0], device='cuda:0')\n",
      "tensor([ 0, 31, 91, 81,  2, 15, 92, 92, 31,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0], device='cuda:0')\n",
      "tensor([ 0, 31, 91, 81,  2, 15, 92, 92, 31, 40,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0], device='cuda:0')\n",
      "tensor([ 0, 31, 91, 81,  2, 15, 92, 92, 31, 40, 33,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0], device='cuda:0')\n",
      "tensor([ 0, 31, 91, 81,  2, 15, 92, 92, 31, 40, 33, 77,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0], device='cuda:0')\n",
      "tensor([ 0, 31, 91, 81,  2, 15, 92, 92, 31, 40, 33, 77, 92,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0, 31, 91, 81,  2, 15, 92, 92, 31, 40, 33, 77, 92, 92,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0], device='cuda:0')\n",
      "tensor([ 0, 31, 91, 81,  2, 15, 92, 92, 31, 40, 33, 77, 92, 92, 92,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0], device='cuda:0')\n",
      "tensor([ 0, 31, 91, 81,  2, 15, 92, 92, 31, 40, 33, 77, 92, 92, 92, 65,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0], device='cuda:0')\n",
      "tensor([ 0, 31, 91, 81,  2, 15, 92, 92, 31, 40, 33, 77, 92, 92, 92, 65, 73,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0], device='cuda:0')\n",
      "tensor([ 0, 31, 91, 81,  2, 15, 92, 92, 31, 40, 33, 77, 92, 92, 92, 65, 73, 40,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0], device='cuda:0')\n",
      "tensor([ 0, 31, 91, 81,  2, 15, 92, 92, 31, 40, 33, 77, 92, 92, 92, 65, 73, 40,\n",
      "        92,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0], device='cuda:0')\n",
      "tensor([ 0, 31, 91, 81,  2, 15, 92, 92, 31, 40, 33, 77, 92, 92, 92, 65, 73, 40,\n",
      "        92, 92,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "src_padding_mask = padding_mask(input_sentence)\n",
    "s=1\n",
    "generated_letter = 0 #we need to put something as the output in the model\n",
    "while (len(generated_sentence))<m+1 and (generated_letter!=char2index['<EOS>']):\n",
    "    tgt_padding_mask = padding_mask(output_sentence)\n",
    "    tgt_padding_mask[0]=False #a fully True mask causes NaN values in the softmax layer\n",
    "    print(output_sentence)\n",
    "    last_output = transformer(input_sentence, output_sentence, src_padding_mask, tgt_padding_mask, attn_mask, pos_encoding)\n",
    "    last_output = torch.argmax(last_output, -1)\n",
    "    generated_letter = last_output[s]\n",
    "    generated_sentence.append(generated_letter)\n",
    "    output_sentence[s] = generated_letter #loop over the generated sentence, character after character\n",
    "    s+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f98ef486",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ó', '\\n', 'î', 'H', 'A', '—', '—', 'ó', 'C', ')', 'P', '—', '—', '—', ';', '²', 'C', '—', '—', '—']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ó\\nîHA——óC)P———;²C———'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "L=[]\n",
    "for c in output_sentence:\n",
    "    if c.item()!=0:\n",
    "        L.append(index2char[c.item()])\n",
    "print(L)\n",
    "''.join(L) #print the generated sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "09286663",
   "metadata": {},
   "outputs": [],
   "source": [
    "char2index['']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dcac304e",
   "metadata": {},
   "outputs": [],
   "source": [
    "index2char[0]=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2120147f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(transformer.state_dict(), \".\\saved_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec82604",
   "metadata": {},
   "source": [
    "Note: I can only train the model for a few hours, it is relatively small and I have very little available data. Obviously, the use of transfer training would have been preferrable here, because the results are really bad (no metrics needed to realise this). I just did this as a fun project to use pytorch a bit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
